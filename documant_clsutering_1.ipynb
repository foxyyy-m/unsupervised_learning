{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "qQzi3OVIX7kr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d crawford/20-newsgroups"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wm9XvBFYC9c",
        "outputId": "cd8f41cb-3af6-4844-ad5a-a58b5e71b530"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 20-newsgroups.zip to /content\n",
            "\r  0% 0.00/25.7M [00:00<?, ?B/s]\r 35% 9.00M/25.7M [00:00<00:00, 67.4MB/s]\n",
            "\r100% 25.7M/25.7M [00:00<00:00, 120MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip 20-newsgroups.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDujn4pnYvJI",
        "outputId": "e4fe0571-e507-4e3b-c84e-cde819797e0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  20-newsgroups.zip\n",
            "  inflating: alt.atheism.txt         \n",
            "  inflating: comp.graphics.txt       \n",
            "  inflating: comp.os.ms-windows.misc.txt  \n",
            "  inflating: comp.sys.ibm.pc.hardware.txt  \n",
            "  inflating: comp.sys.mac.hardware.txt  \n",
            "  inflating: comp.windows.x.txt      \n",
            "  inflating: list.csv                \n",
            "  inflating: misc.forsale.txt        \n",
            "  inflating: rec.autos.txt           \n",
            "  inflating: rec.motorcycles.txt     \n",
            "  inflating: rec.sport.baseball.txt  \n",
            "  inflating: rec.sport.hockey.txt    \n",
            "  inflating: sci.crypt.txt           \n",
            "  inflating: sci.electronics.txt     \n",
            "  inflating: sci.med.txt             \n",
            "  inflating: sci.space.txt           \n",
            "  inflating: soc.religion.christian.txt  \n",
            "  inflating: talk.politics.guns.txt  \n",
            "  inflating: talk.politics.mideast.txt  \n",
            "  inflating: talk.politics.misc.txt  \n",
            "  inflating: talk.religion.misc.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
        "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import sys\n",
        "from time import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:25.708885Z",
          "iopub.execute_input": "2023-06-28T23:16:25.709265Z",
          "iopub.status.idle": "2023-06-28T23:16:27.090912Z",
          "shell.execute_reply.started": "2023-06-28T23:16:25.709227Z",
          "shell.execute_reply": "2023-06-28T23:16:27.089798Z"
        },
        "trusted": true,
        "id": "6aotNW9qX6Bo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFcroVsdX-K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:27.278139Z",
          "iopub.execute_input": "2023-06-28T23:16:27.278590Z",
          "iopub.status.idle": "2023-06-28T23:16:27.283625Z",
          "shell.execute_reply.started": "2023-06-28T23:16:27.278551Z",
          "shell.execute_reply": "2023-06-28T23:16:27.282755Z"
        },
        "trusted": true,
        "id": "-n_4-2jWX6Bq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'fetch_20newsgroups' is a function in the scikit-learn library (a popular machine learning library in Python) that allows you to load the 20 Newsgroups dataset. The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents across 20 different topics or categories."
      ],
      "metadata": {
        "id": "DuUBypL0X6Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    'rec.sport.baseball',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'sci.space',\n",
        "]\n",
        "\n",
        "print(\"Loading 20 newsgroups dataset for categories:\")\n",
        "print(categories)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:27.284930Z",
          "iopub.execute_input": "2023-06-28T23:16:27.285551Z",
          "iopub.status.idle": "2023-06-28T23:16:27.299230Z",
          "shell.execute_reply.started": "2023-06-28T23:16:27.285522Z",
          "shell.execute_reply": "2023-06-28T23:16:27.297832Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC0SfAFrX6Bs",
        "outputId": "7512bc1f-e280-4963-9eb5-c74875ef2985"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 20 newsgroups dataset for categories:\n",
            "['rec.sport.baseball', 'comp.sys.ibm.pc.hardware', 'sci.space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = fetch_20newsgroups(subset='train', categories=categories,\n",
        "                             shuffle=False, remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:27.301161Z",
          "iopub.execute_input": "2023-06-28T23:16:27.301746Z",
          "iopub.status.idle": "2023-06-28T23:16:45.724474Z",
          "shell.execute_reply.started": "2023-06-28T23:16:27.301706Z",
          "shell.execute_reply": "2023-06-28T23:16:45.723418Z"
        },
        "trusted": true,
        "id": "0rD4gFegX6Bs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function fetch_20newsgroups retrieves the dataset and returns a dictionary-like object with the following attributes:\n",
        "\n",
        "data: A list of strings containing the text of the newsgroup posts.\n",
        "target: An array of integers representing the category labels for each document.\n",
        "target_names: A list of strings representing the category names or labels.\n",
        "DESCR: A description of the dataset."
      ],
      "metadata": {
        "id": "2rJIxS0QX6Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.keys()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:45.726003Z",
          "iopub.execute_input": "2023-06-28T23:16:45.726316Z",
          "iopub.status.idle": "2023-06-28T23:16:45.736714Z",
          "shell.execute_reply.started": "2023-06-28T23:16:45.726289Z",
          "shell.execute_reply": "2023-06-28T23:16:45.735745Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFbz3e4TX6Bt",
        "outputId": "0d211642-7693-401e-a269-c0b6b9e51807"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.data[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:45.737855Z",
          "iopub.execute_input": "2023-06-28T23:16:45.738133Z",
          "iopub.status.idle": "2023-06-28T23:16:45.794227Z",
          "shell.execute_reply.started": "2023-06-28T23:16:45.738109Z",
          "shell.execute_reply": "2023-06-28T23:16:45.793391Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LYKQNIGQX6Bt",
        "outputId": "3f1e986e-6c72-4e9d-ac10-5a42c8e4d868"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nmorgan and guzman will have era's 1 run higher than last year, and\\n the cubs will be idiots and not pitch harkey as much as hibbard.\\n castillo won't be good (i think he's a stud pitcher)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.target[0] , df.target_names[0] , df.filenames[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:45.795520Z",
          "iopub.execute_input": "2023-06-28T23:16:45.796650Z",
          "iopub.status.idle": "2023-06-28T23:16:45.807814Z",
          "shell.execute_reply.started": "2023-06-28T23:16:45.796610Z",
          "shell.execute_reply": "2023-06-28T23:16:45.806939Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8HX9vaIX6Bt",
        "outputId": "3ed3df12-745d-4225-feed-dea37bca7161"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " '/root/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.baseball/102736')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df.target"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:45.812322Z",
          "iopub.execute_input": "2023-06-28T23:16:45.812706Z",
          "iopub.status.idle": "2023-06-28T23:16:45.818805Z",
          "shell.execute_reply.started": "2023-06-28T23:16:45.812674Z",
          "shell.execute_reply": "2023-06-28T23:16:45.817941Z"
        },
        "trusted": true,
        "id": "b2jHDZSSX6Bt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels) , len(df.data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:45.820145Z",
          "iopub.execute_input": "2023-06-28T23:16:45.821131Z",
          "iopub.status.idle": "2023-06-28T23:16:45.834556Z",
          "shell.execute_reply.started": "2023-06-28T23:16:45.821096Z",
          "shell.execute_reply": "2023-06-28T23:16:45.833244Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OXoqVrQX6Bt",
        "outputId": "7b99e4f6-a909-4d37-fcdb-3fb13850b0f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1780, 1780)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizing: Lemmatizing is the process of reducing words to their base or dictionary form, known as the lemma. It involves considering the context and morphological analysis of words to determine their base form. For example, the lemma of \"running,\" \"runs,\" and \"ran\" is \"run.\"\n",
        "\n",
        "Lemmatizing limitations:\n",
        "\n",
        "Lemmatizing requires access to comprehensive linguistic resources or dictionaries.\n",
        "It may struggle with irregular or domain-specific terms that are not present in the resources.\n",
        "Lemmatizing can be slower and computationally more intensive compared to stemming.\n",
        "Stemming limitations:\n",
        "\n",
        "Stemming relies on simple rules and heuristics, so it may produce base forms that are not actual words (e.g., \"comput\" instead of \"compute\").\n",
        "Stemming does not consider the context or part of speech of the word, which can result in less accurate base forms.\n",
        "Stemming can sometimes over-stem, collapsing different words with the same root but different meanings (e.g., \"play\" and \"playing\" both becoming \"play\").\n",
        "\n",
        "In general, lemmatizing is often preferred for document clustering because it retains the linguistic accuracy and context of words. By reducing words to their base forms, lemmatizing helps to group together documents that contain related or similar concepts, even if they use different inflected forms of words"
      ],
      "metadata": {
        "id": "Lj6CJsVzX6Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:16:46.087642Z",
          "iopub.execute_input": "2023-06-28T23:16:46.087991Z",
          "iopub.status.idle": "2023-06-28T23:17:00.137893Z",
          "shell.execute_reply.started": "2023-06-28T23:16:46.087960Z",
          "shell.execute_reply": "2023-06-28T23:17:00.136486Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMku3iQFX6Bu",
        "outputId": "6e4d1b8a-a249-4518-f4b6-6a03035e8059"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:17:00.139683Z",
          "iopub.execute_input": "2023-06-28T23:17:00.140039Z",
          "iopub.status.idle": "2023-06-28T23:17:29.966948Z",
          "shell.execute_reply.started": "2023-06-28T23:17:00.140007Z",
          "shell.execute_reply": "2023-06-28T23:17:29.965576Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhbKiyIYX6Bu",
        "outputId": "14518896-d0df-4c2a-dbf4-c3bf4c754b35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-28 23:43:52.919312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-28 23:43:56.919128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Load the English model in SpaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a function for lemmatization\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
        "    return lemmatized_text\n",
        "\n",
        "# Example usage\n",
        "for i in range(len(df.data)):\n",
        "    df.data[i] = lemmatize_text(df.data[i])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:17:29.970914Z",
          "iopub.execute_input": "2023-06-28T23:17:29.971255Z",
          "iopub.status.idle": "2023-06-28T23:19:42.920074Z",
          "shell.execute_reply.started": "2023-06-28T23:17:29.971224Z",
          "shell.execute_reply": "2023-06-28T23:19:42.918958Z"
        },
        "trusted": true,
        "id": "9nZenWUbX6Bu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.data[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:20:24.724535Z",
          "iopub.execute_input": "2023-06-28T23:20:24.724914Z",
          "iopub.status.idle": "2023-06-28T23:20:24.730669Z",
          "shell.execute_reply.started": "2023-06-28T23:20:24.724886Z",
          "shell.execute_reply": "2023-06-28T23:20:24.729767Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnKQjNRoX6Bu",
        "outputId": "5b9c2890-ddd1-427e-e9d9-12372e432ae2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " morgan and guzman will have era 's 1 run high than last year , and \n",
            "  the cub will be idiot and not pitch harkey as much as hibbard . \n",
            "  castillo will not be good ( I think he be a stud pitcher )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', min_df=2) ## Corpus is in English\n",
        "X = vectorizer.fit_transform(df.data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:23:36.275449Z",
          "iopub.execute_input": "2023-06-28T23:23:36.275848Z",
          "iopub.status.idle": "2023-06-28T23:23:36.808990Z",
          "shell.execute_reply.started": "2023-06-28T23:23:36.275820Z",
          "shell.execute_reply": "2023-06-28T23:23:36.807884Z"
        },
        "trusted": true,
        "id": "np49XGb2X6Bu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF vectorizer stands for Term Frequency-Inverse Document Frequency vectorizer. It is used to convert text documents into numerical vectors based on the importance of each word in the document and the entire corpus. It assigns higher weights to words that appear frequently in a document but infrequently in the entire corpus, capturing their relative importance. It is commonly used in natural language processing tasks, such as document classification and information retrieval."
      ],
      "metadata": {
        "id": "eh0pkzxlX6Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following three documents:\n",
        "\n",
        "Document 1: \"I love cats.\"\n",
        "Document 2: \"I love dogs.\"\n",
        "Document 3: \"I hate spiders.\"\n",
        "\n",
        "To convert these documents into TF-IDF vectors, we follow these steps:\n",
        "\n",
        "Term Frequency (TF): Calculate the frequency of each word in each document. For example, in Document 1, the word \"I\" appears once, \"love\" appears once, and \"cats\" appears once.\n",
        "\n",
        "Inverse Document Frequency (IDF): Calculate the inverse document frequency of each word. This measures how important a word is in the entire corpus. Words that appear frequently across the corpus have lower IDF values, while words that appear in fewer documents have higher IDF values.\n",
        "\n",
        "TF-IDF Calculation: Multiply the term frequency (TF) of each word in a document by its inverse document frequency (IDF). This calculates the TF-IDF value for each word in each document.\n",
        "\n",
        "For example, let's assume that the IDF values for \"I\" is 0.3, \"love\" is 0.2, \"cats\" is 0.5, \"dogs\" is 0.5, and \"spiders\" is 0.5.\n",
        "\n",
        "The TF-IDF vectors for the three documents would be:\n",
        "\n",
        "Document 1: [0.3, 0.2, 0.5, 0, 0]\n",
        "Document 2: [0.3, 0.2, 0, 0.5, 0]\n",
        "Document 3: [0.3, 0, 0, 0, 0.5]\n",
        "\n",
        "In this example, the TF-IDF vector representation captures the importance of each word in the documents. Words that are unique to a document or have higher importance within that document receive higher TF-IDF values. This allows us to compare and analyze the documents based on their textual content using numerical vectors."
      ],
      "metadata": {
        "id": "Bsw3rEnDX6Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:19:49.554427Z",
          "iopub.execute_input": "2023-06-28T23:19:49.554785Z",
          "iopub.status.idle": "2023-06-28T23:19:49.560751Z",
          "shell.execute_reply.started": "2023-06-28T23:19:49.554758Z",
          "shell.execute_reply": "2023-06-28T23:19:49.559942Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkYUjT61X6Bv",
        "outputId": "ea5facc6-3060-4ced-e9a4-7bbae1251524"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1780, 8179)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:19:54.983824Z",
          "iopub.execute_input": "2023-06-28T23:19:54.984218Z",
          "iopub.status.idle": "2023-06-28T23:19:54.990549Z",
          "shell.execute_reply.started": "2023-06-28T23:19:54.984186Z",
          "shell.execute_reply": "2023-06-28T23:19:54.989855Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7axrcdmqX6Bv",
        "outputId": "9b36f5b6-e0bc-486b-e398-c8c0bc3394c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x8179 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 15 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K_MEANS++**"
      ],
      "metadata": {
        "id": "pe2FaRpKX6Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=3, init='k-means++', max_iter=100)\n",
        "t0 = time()\n",
        "km.fit(X)\n",
        "print(\"done in %0.3fs\" % (time() - t0))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:20:34.266644Z",
          "iopub.execute_input": "2023-06-28T23:20:34.267024Z",
          "iopub.status.idle": "2023-06-28T23:20:34.704064Z",
          "shell.execute_reply.started": "2023-06-28T23:20:34.266997Z",
          "shell.execute_reply": "2023-06-28T23:20:34.703204Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdAZqffzX6Bv",
        "outputId": "203567b7-c7bb-40f5-bdf6-44afa4d3c08f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done in 0.276s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
        "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
        "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
        "print(\"Adjusted Rand-Index: %.3f\"\n",
        "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
        "print(\"Silhouette Coefficient: %0.3f\"\n",
        "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:20:39.058765Z",
          "iopub.execute_input": "2023-06-28T23:20:39.059827Z",
          "iopub.status.idle": "2023-06-28T23:20:39.121880Z",
          "shell.execute_reply.started": "2023-06-28T23:20:39.059794Z",
          "shell.execute_reply": "2023-06-28T23:20:39.120859Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ0JbmkXX6Bv",
        "outputId": "987feccc-2674-4b76-9ae7-2ed337b2b9eb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homogeneity: 0.342\n",
            "Completeness: 0.464\n",
            "V-measure: 0.394\n",
            "Adjusted Rand-Index: 0.235\n",
            "Silhouette Coefficient: 0.012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i in range(3):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in centroids[i, :10]:\n",
        "        print(' %s' % terms[ind], end='')\n",
        "    print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-28T23:24:40.756017Z",
          "iopub.execute_input": "2023-06-28T23:24:40.756475Z",
          "iopub.status.idle": "2023-06-28T23:24:40.775260Z",
          "shell.execute_reply.started": "2023-06-28T23:24:40.756437Z",
          "shell.execute_reply": "2023-06-28T23:24:40.773959Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TONOgDrcX6Bv",
        "outputId": "7ce0ea87-d218-40b9-cf95-2e655c65c90d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: game year team player win run good hit think baseball\n",
            "Cluster 1: space use know just thank like card think time problem\n",
            "Cluster 2: drive scsi controller ide disk bus hard floppy mb use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# # Assuming you have your data and labels\n",
        "# X = df.data\n",
        "# labels = labels\n",
        "\n",
        "# # Apply dimensionality reduction using PCA\n",
        "# pca = PCA(n_components=2)\n",
        "# X_pca = pca.fit_transform(X)\n",
        "\n",
        "# # Fit the K-means model\n",
        "# kmeans = KMeans(n_clusters=3)\n",
        "# kmeans.fit(X)\n",
        "\n",
        "# # Plot the clusters\n",
        "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis')\n",
        "# plt.title(\"K-means Clustering\")\n",
        "# plt.xlabel(\"PC1\")\n",
        "# plt.ylabel(\"PC2\")\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "6Z1-Mj8GX6Bv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZw0AmdwX6Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1dEvqyIX6Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F76gKBfIX6Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLCkCgX5X6Bw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}